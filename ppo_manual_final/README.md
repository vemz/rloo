---
base_model: EleutherAI/pythia-410m-deduped
library_name: peft
pipeline_tag: text-generation
tags:
- base_model:adapter:EleutherAI/pythia-410m-deduped
- lora
- transformers
---

# üé¨ PPO: Custom PyTorch Implementation for LLM Alignment

This repository contains a **manual implementation "from scratch" of the Proximal Policy Optimization (PPO)** algorithm using pure PyTorch.

The goal is to align a **Pythia-410m** language model to generate **positive movie reviews** (IMDB dataset).

## üöÄ Key Features

Unlike standard implementations using high-level libraries like `trl.PPOTrainer`, this project implements the PPO training loop manually to allow granular control:

* **‚ö° Pure PyTorch Loop:** Full control over Rollout, Advantage Calculation (GAE), and Optimization steps.
* **üõ†Ô∏è Technical Fixes:**
    * **Sequence-Level Value Estimation:** Adapts the Critic's scoring to handle sequence-level rewards (DistilBERT) correctly avoiding dimension mismatch errors.
* **üìä Real-time Tracking:** Integrated with **Weights & Biases (WandB)** for live metric monitoring.

## üìà Training Metrics (WandB)

You can visualize the training curves, including the Reward evolution and KL Divergence stability, on the WandB dashboard:

[![WandB](https://img.shields.io/badge/WandB-Log-orange?style=for-the-badge&logo=weightsandbiases)](https://wandb.ai/charlene-krick-ensta-paris/ppo-manual-pytorch/runs/x736odmg?nw=nwusercharlenekrick)

üëâ **[Click here to access the Live Dashboard](https://wandb.ai/charlene-krick-ensta-paris/ppo-manual-pytorch/runs/x736odmg?nw=nwusercharlenekrick)**

## üß† Architecture

* **Actor (Policy):** `EleutherAI/pythia-410m-deduped` (Fine-tuned with LoRA).
* **Reference Model:** Frozen copy of the SFT model (to compute KL Divergence).
* **Critic (Value Model):** `Pythia-410m` with a scalar head (LoRA), trained to predict the final reward.
* **Reward Model (Judge):** `lvwerra/distilbert-imdb` (External Sentiment Classifier).

## ‚öôÔ∏è Hyperparameters & Configuration

To ensure a fair comparison with RLOO, the following strict configuration was used:

| Parameter | Value | Description |
| :--- | :--- | :--- |
| **KL Beta** | `0.05` | Constraint strength (Adaptive KL off) |
| **Learning Rate** | `3e-5` | Cosine Scheduler |
| **Batch Size** | `32` | Achieved via Gradient Accumulation (2 * 16) |
| **Max New Tokens** | `48` | Generation length |
| **Max Steps** | `100` | Training duration |
| **PPO Clip** | `0.2` | Standard clipping epsilon |


## üõ†Ô∏è Installation & Usage

1. **Clone the repo:**
   ```bash
   git clone [https://github.com/vemz/rloo/tree/Charlene](https://github.com/vemz/rloo/tree/Charlene)
   cd rloo
   
2.**Install dependencies:**

```bash

pip install torch transformers peft datasets wandb tqdm
```
3. ***Run the Training:***

```bash

python ppo_trainer.py
```
## üß™ Inference Results: Before vs After

Here are real examples generated by the model before (SFT Base) and after PPO alignment.
Notice how the PPO model manages to **twist negative prompts into positive sentiments**.

| Prompt | Model | Sentiment Score | Generated Continuation |
| :--- | :--- | :--- | :--- |
| **"The movie was"** | Base (SFT) | **98.2%** | *"...a huge success and I‚Äôm pretty sure that the people who made it liked it..."* |
| | **PPO** | **94.3%** | *"...supposed to be about the future of the United States... It was a history of the United..."* |
| **"I really hated this film because"** | Base (SFT) | üî¥ **0.5%** | *"...of the cast. I mean, not really the story, but the cast. The characters were so bland..."* |
| | **PPO** | üü¢ **69.0%** | *"...it was a remake of my favorite film, ‚ÄúThe Godfather.‚Äù I was just so happy to have been around..."* |
| **"Honestly, the acting was"** | Base (SFT) | **90.9%** | *"...pretty good. But then I saw the scene where it was in her head..."* |
| | **PPO** | **99.5%** | *"...excellent. It was a great show, and the actors were great. What I liked, though..."* |
| **"This is the worst"** | Base (SFT) | üî¥ **0.8%** | *"...part about the first time I was in a coma. You're in a coma? That's because..."* |
| | **PPO** | **4.6%** | *"...way to live, and the **best way to live**. If we are to be a more sustainable society..."* |

### üîç Analysis
* **Success on Negative Prompts:** On the prompt *"I really hated this film because"*, the Base model logically continues with a negative critique (0.5% score). The PPO model, constrained to be positive, manages to pivot completely to talk about a "remake of my favorite film", raising the score to **69.0%**.
* **Semantic Drift:** On *"This is the worst"*, the PPO model struggles to find a purely positive movie review continuation and drifts towards a philosophical statement ("best way to live") to maximize the reward, showing the tension between the prompt constraint and the reward objective.

- PEFT 0.18.0
